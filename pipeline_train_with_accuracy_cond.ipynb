{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50c508a",
   "metadata": {},
   "source": [
    "# Adding a Conditional Step to Evaluate the Model Performance in AWS SageMaker Pipelines\n",
    "## Training Pipeline - Using Bring your own code method\n",
    "### Usecase- Customer Churn Prediction \n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f36738",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [1. Data](#1.-Data)\n",
    "\t* [1.1 Importing data](#1.1-Importing-data)\n",
    "\t* [1.2 Data description](#1.2-Data-description)\n",
    "\t* [1.3 EDA](#1.3-EDA)    \n",
    "* [2. Sagemaker Pipeline](#2.-Sagemaker-Pipeline)\n",
    "\t* [2.1 Architecture](#2.1-Architecture)\n",
    "\t* [2.2 Install predefined Sagemaker libraries](#2.2-Install-predefined-Sagemaker-libraries)\n",
    "\t* [2.3 Import MLOps define functions](#2.3-Import-MLOps-define-functions)\n",
    "\t* [2.4 Define Preprocessing Stage](#2.4-Define-Preprocessing-Stage)\n",
    "\t* [2.5 Define Training stage](#2.5-Define-Training-stage)\n",
    "\t* [2.6 Define the Model Evaluation step](#2.5-Define-the-Model-Evaluation-step)\n",
    "\t* [2.7 Define the Model Register step](#2.6-Define-the-Model-Register-step)\n",
    "\t* [2.7 Define the Condition Step](#2.6-Define-the-Condition-Step)\n",
    "\t* [2.9 Define required parameters for get pipeline](#2.7-Define-required-parameters-for-get-pipeline)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b3991",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836255d",
   "metadata": {},
   "source": [
    "## 1.1 Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c085c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
       "0  No phone service             DSL             No  ...               No   \n",
       "1                No             DSL            Yes  ...              Yes   \n",
       "2                No             DSL            Yes  ...               No   \n",
       "3  No phone service             DSL            Yes  ...              Yes   \n",
       "4                No     Fiber optic             No  ...               No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
       "0           Electronic check          29.85         29.85    No  \n",
       "1               Mailed check          56.95        1889.5    No  \n",
       "2               Mailed check          53.85        108.15   Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
       "4           Electronic check          70.70        151.65   Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/customer_churn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a554d8f",
   "metadata": {},
   "source": [
    "## 1.2 Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a8b1d",
   "metadata": {},
   "source": [
    "Each row represents a customer, each column contains customer’s attributes described on the column Metadata. The raw data contains 7043 rows (customers) and 21 columns (features). The “Churn” column is our target. \n",
    "\n",
    "- **`CustomerID:`** Customer ID \n",
    "\n",
    "- **`Gender:`** Whether the customer is a male or a female \n",
    "\n",
    "- **`SeniorCitizen:`** Whether the customer is a senior citizen or not (1, 0) \n",
    "\n",
    "- **`Partner:`** Whether the customer has a partner or not (Yes, No) \n",
    "\n",
    "- **`Dependents:`** Whether the customer has dependents or not (Yes, No) \n",
    "\n",
    "- **`tenure:`** Number of months the customer has stayed with the company \n",
    "\n",
    "- **`PhoneService:`** Whether the customer has a phone service or not (Yes, No) \n",
    "\n",
    "- **`MultipleLines:`** Whether the customer has multiple lines or not (Yes, No, No phone service) \n",
    "\n",
    "- **`InternetService:`** Customer’s internet service provider (DSL, Fiber optic, No) \n",
    "\n",
    "- **`OnlineSecurity:`** Whether the customer has online security or not (Yes, No, No internet service) \n",
    "\n",
    "- **`OnlineBackup:`** Whether the customer has online backup or not (Yes, No, No internet service)\n",
    "\n",
    "- **`DeviceProtection:`** Whether the customer has device protection or not (Yes, No, No internet service)\n",
    "\n",
    "- **`TechSupport:`** Whether the customer has tech support or not (Yes, No, No internet service)\n",
    "\n",
    "- **`StreamingTV:`** Whether the customer has streaming TV or not (Yes, No, No internet service)\n",
    "\n",
    "- **`StreamingMovies:`** Whether the customer has streaming movies or not (Yes, No, No internet service)\n",
    "\n",
    "- **`Contract:`** The contract term of the customer (Month-to-month, One year, Two year)\n",
    "\n",
    "- **`PaperlessBilling:`** Whether the customer has paperless billing or not (Yes, No)\n",
    "\n",
    "- **`PaymentMethod:`** The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n",
    "\n",
    "- **`MonthlyCharges:`** The amount charged to the customer monthly\n",
    "\n",
    "- **`TotalCharges:`** The total amount charged to the customer\n",
    "\n",
    "- **`Churn:`** Whether the customer churned or not (Yes or No)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1999b2f",
   "metadata": {},
   "source": [
    "### 1.3 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6d3e8",
   "metadata": {},
   "source": [
    "If you need to do further analysis about the data, please follow the Auto EDA library (Pandas Profiling)\n",
    "```python\n",
    "pip install pandas-profiling\n",
    "\n",
    "#importing required packages\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "\n",
    "#descriptive statistics\n",
    "pandas_profiling.ProfileReport(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba929847",
   "metadata": {},
   "source": [
    "# 2. Sagemaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0fb159",
   "metadata": {},
   "source": [
    "SageMaker Pipelines supports the following activities, which are demonstrated in this notebook:\n",
    "\n",
    "- Pipelines - A DAG of steps and conditions to orchestrate SageMaker jobs and resource creation.\n",
    "- Processing job steps - A simplified, managed experience on SageMaker to run data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation.\n",
    "- Training job steps - An iterative process that teaches a model to make predictions by presenting examples from a training dataset.\n",
    "- Register model steps - A step that creates a model package resource in the Model Registry that can be used to create deployable models in Amazon SageMaker.\n",
    "- Create model steps - A step that creates a model for use in transform steps or later publication as an endpoint.\n",
    "- Transform job steps - A batch transform to preprocess datasets to remove noise or bias that interferes with training or inference from a dataset, get inferences from large datasets, and run inference when a persistent endpoint is not needed.\n",
    "- Post processing - (Optional) A step that filtering the final predicted output base : In here we don't include that step into our pipeline\n",
    "- Parametrized Pipeline executions - Enables variation in pipeline executions according to specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302aa051",
   "metadata": {},
   "source": [
    "### 2.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13afd96",
   "metadata": {},
   "source": [
    "This **training** pipeline contains preprocess, training and model register steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450bcf5",
   "metadata": {},
   "source": [
    "![architecture](images/eval11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb3f13",
   "metadata": {},
   "source": [
    "Lets beginning the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3802a67",
   "metadata": {},
   "source": [
    "![workflow](images/eval1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317e1c9",
   "metadata": {},
   "source": [
    "## 2.2 Install predefined Sagemaker libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd906e1",
   "metadata": {},
   "source": [
    "Initailly we have to install AWS predefined Sagemaker libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7d0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput,TransformInput,CreateModelInput\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.conditions import (\n",
    "    ConditionGreaterThanOrEqualTo,\n",
    ")\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    TransformStep,\n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b6264",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48df667",
   "metadata": {},
   "source": [
    "## 2.3 Import Other define functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2f7ba",
   "metadata": {},
   "source": [
    "Next step is start session and in this process we are defining our <b> AWS region, sagemaker client, boto 3 session</b> and <b>default s3 bucket </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35943cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(region, default_bucket):\n",
    "    \"\"\"Gets the sagemaker session based on the region.\n",
    "    Args:\n",
    "        region: the aws region to start the session\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        `sagemaker.session.Session instance\n",
    "    \"\"\"\n",
    "\n",
    "    boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "    sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "    runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "    return sagemaker.session.Session(\n",
    "        boto_session=boto_session,\n",
    "        sagemaker_client=sagemaker_client,\n",
    "        sagemaker_runtime_client=runtime_client,\n",
    "        default_bucket=default_bucket,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed18533",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_bucket = sess.default_bucket()\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "region = boto3.session.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f434c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store this dataset in the s3 bucket\n",
    "df.to_csv(f\"s3://{default_bucket}/customer_churn/inference/training_input_dataset/telco_cutomer_churn.csv\", header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f3662d",
   "metadata": {},
   "source": [
    "Enter the project name for import config files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c4cfd",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f0b38",
   "metadata": {},
   "source": [
    "## 2.4 Define Preprocessing Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023181e",
   "metadata": {},
   "source": [
    "This is the script used in preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31c7fa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "warnings.filterwarnings(\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category=\u001b[36mFutureWarning\u001b[39;49;00m)\n",
      "warnings.filterwarnings(\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category=\u001b[36mDeprecationWarning\u001b[39;49;00m)\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "warnings.simplefilter(action=\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MinMaxScaler\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mchange_format\u001b[39;49;00m(df):\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = pd.to_numeric(df.TotalCharges, errors=\u001b[33m'\u001b[39;49;00m\u001b[33mcoerce\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmissing_value\u001b[39;49;00m(df):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcount of missing values: (before treatment)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, df.isnull().sum())\n",
      "    \n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].fillna(df[\u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].mean())\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcount of missing values: (before treatment)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, df.isnull().sum())\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmissing values successfully replaced\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdata_manipulation\u001b[39;49;00m(df):\n",
      "    df = df.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis = \u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcat_encoder\u001b[39;49;00m(df, variable_list):\n",
      "    dummy = pd.get_dummies(df[variable_list], drop_first = \u001b[34mTrue\u001b[39;49;00m)\n",
      "    df = pd.concat([df, dummy], axis=\u001b[34m1\u001b[39;49;00m)\n",
      "    df.drop(df[cat_var], axis = \u001b[34m1\u001b[39;49;00m, inplace = \u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEncoded successfully\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mscaling\u001b[39;49;00m(X):  \n",
      "    min_max=MinMaxScaler()\n",
      "    X=pd.DataFrame(min_max.fit_transform(X),columns=X.columns)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m X\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "\n",
      "    input_data_path = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtelco_cutomer_churn.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReading input data from \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_data_path))\n",
      "    df = pd.read_csv(input_data_path)\n",
      "    \n",
      "    columns = [\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mgender\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mSeniorCitizen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPartner\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDependents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mtenure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPhoneService\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mContract\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaperlessBilling\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMonthlyCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mChurn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    df.columns = columns\n",
      "\n",
      "    cat_var = [\u001b[33m'\u001b[39;49;00m\u001b[33mgender\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPartner\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDependents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPhoneService\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mContract\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaperlessBilling\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mChurn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    df = data_manipulation(missing_value(change_format(df)))\n",
      "    df = cat_encoder(df, cat_var)\n",
      "\n",
      "    X = df.iloc[:, \u001b[34m0\u001b[39;49;00m:\u001b[34m30\u001b[39;49;00m]\n",
      "    y = df.iloc[:, -\u001b[34m1\u001b[39;49;00m]\n",
      "    X = scaling(X)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the outputs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    X_output_path = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mX.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)   \n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving output to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(X_output_path))\n",
      "    pd.DataFrame(X).to_csv(X_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    y_output_path = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33my.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)   \n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving output to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(y_output_path))\n",
      "    pd.DataFrame(y).to_csv(y_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize customer_churn_training_preprocessing/preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c115b0",
   "metadata": {},
   "source": [
    "This is the code for run the **preprocessing** part. You can add more inputs or outputs according to your requirement. Please refer the guideline document for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b8e3a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(\n",
    "    region,\n",
    "    subnets,\n",
    "    security_group_ids,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"CustomerChurnModelPackageGroup\",  # Choose any name\n",
    "    pipeline_name=\"customer-churn-prediction-training-pipeline\",  # You can find your pipeline name in the Studio UI (project -> Pipelines -> name)\n",
    "    base_job_prefix=\"cutomer-churn-prediction-training\", # Choose any name\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on CustomerChurn data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    #data versioning control using date\n",
    "    srilanka_tz = pytz.timezone('Asia/Colombo')\n",
    "    s3 = boto3.client('s3')\n",
    "    date_folder = datetime.now(srilanka_tz).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #working with input data path\n",
    "    input_data =  f\"s3://{default_bucket}/customer_churn/training_input_dataset/telco_cutomer_churn.csv\"\n",
    "    \n",
    "    #working with output data path   \n",
    "    preprocessed_output1 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output1/\"\n",
    "    preprocessed_output2 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output2/\"\n",
    "    \n",
    "    \n",
    "    sagemaker_session = get_session(region, default_bucket)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    # Parameters for pipeline execution\n",
    "    model_path = ParameterString(\n",
    "        name=\"ModelPath\",\n",
    "        default_value=f\"s3://{default_bucket}/customer_churn/training/{date_folder}/model/xgboost/\", \n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\",\n",
    "        default_value=\"PendingManualApproval\",  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    "    )\n",
    "\n",
    "    ####### --------------------- PREPROCESSING --------------------------------------------------------------------\n",
    "\n",
    "    ecr_repository = \"customer-churn-prediction-preprocessing-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    preprocessing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "        \n",
    "    script_processor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = \"ml.m5.large\",\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )\n",
    "    \n",
    "    step_preprocess = ProcessingStep(\n",
    "        name= f\"{base_job_prefix}-preprocessing\",\n",
    "        processor= script_processor, \n",
    "        code= \"customer_churn_training_preprocessing/preprocessing.py\",\n",
    "        inputs= [ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "               ],\n",
    "        outputs= [\n",
    "            ProcessingOutput(output_name=\"output1\", destination=preprocessed_output1, source=\"/opt/ml/processing/output1\"),\n",
    "            ProcessingOutput(output_name=\"output2\", destination=preprocessed_output2,  source=\"/opt/ml/processing/output2\"),       \n",
    "        ]\n",
    "    )\n",
    "    # ========================================= PIPELINE ORCHESTRATION ================================================\n",
    "    \n",
    "    # Pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "        ],\n",
    "        steps=[\n",
    "            step_preprocess,\n",
    "              ],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5302d8",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c41129",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7813b",
   "metadata": {},
   "source": [
    "## 2.5 Define Training stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fccf78",
   "metadata": {},
   "source": [
    "We used this script for model training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60b84c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#Import the neccessary libaries in here\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mxgboost\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m XGBClassifier,plot_importance\n",
      "\u001b[37m#from imblearn.over_sampling import SMOTE\u001b[39;49;00m\n",
      "\u001b[37m#from imblearn.combine import SMOTETomek # doctest: +NORMALIZE_WHITESPACE\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m precision_recall_fscore_support \u001b[34mas\u001b[39;49;00m score\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_score, recall_score, auc,roc_curve,r2_score,confusion_matrix,roc_auc_score,f1_score\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GridSearchCV\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m confusion_matrix , classification_report\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    training_data_directory = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/input1/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    training_data_directory2 = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/input2/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    train_features_data = os.path.join(training_data_directory, \u001b[33m\"\u001b[39;49;00m\u001b[33mX.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    train_labels_data = os.path.join(training_data_directory2, \u001b[33m\"\u001b[39;49;00m\u001b[33my.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReading input data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReading input data from \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_features_data))\n",
      "    X = pd.read_csv(train_features_data, header = \u001b[34mNone\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReading input data from \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_labels_data))\n",
      "    y = pd.read_csv(train_labels_data, header = \u001b[34mNone\u001b[39;49;00m)\n",
      "    \n",
      "    columns = [\u001b[33m'\u001b[39;49;00m\u001b[33mSeniorCitizen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtenure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMonthlyCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mgender_Male\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPartner_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDependents_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPhoneService_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines_No phone service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService_Fiber optic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService_No\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mContract_One year\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mContract_Two year\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaperlessBilling_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Credit card (automatic)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Electronic check\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Mailed check\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \n",
      "    X.columns = columns\n",
      "    \n",
      "    column = [\u001b[33m'\u001b[39;49;00m\u001b[33mChurn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    y.columns = column\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSuccessfully rename the dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msplit the dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=\u001b[34m0.3\u001b[39;49;00m,random_state=\u001b[34m5\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain the model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    xgb = XGBClassifier()\n",
      "    param_grid = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mn_estimators\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[34m100\u001b[39;49;00m, \u001b[34m250\u001b[39;49;00m, \u001b[34m500\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mmax_depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[34m3\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m : [\u001b[34m0.01\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m, \u001b[34m0.1\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mgamma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m : [\u001b[34m0.0\u001b[39;49;00m, \u001b[34m0.1\u001b[39;49;00m, \u001b[34m0.2\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mmin_child_weight\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m : [\u001b[34m1\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m]\n",
      "    }\n",
      "    \n",
      "    cv = GridSearchCV(xgb, param_grid, cv=\u001b[34m3\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfitting the model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    cv.fit(X_train, y_train.values.ravel())\n",
      "\n",
      "    final_model = cv.best_estimator_\n",
      "\n",
      "    y_pred = final_model.predict(X_test)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel evaluation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(confusion_matrix(y_test,final_model.predict(X_test)))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(classification_report(y_test,y_pred))\n",
      "\n",
      "    roc_auc_score=roc_auc_score(y_test,final_model.predict_proba(X_test)[:, \u001b[34m1\u001b[39;49;00m])\n",
      "    precision=precision_score(y_test,y_pred)\n",
      "    recall=recall_score(y_test,y_pred)\n",
      "    f1=f1_score(y_test,y_pred)\n",
      "    accuracy=accuracy_score(y_test, y_pred)\n",
      "\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mroc_auc_score:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mround\u001b[39;49;00m(roc_auc_score, \u001b[34m3\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprecision:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mround\u001b[39;49;00m(precision, \u001b[34m3\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mrecall_score:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mround\u001b[39;49;00m(recall, \u001b[34m3\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mf1_score:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mround\u001b[39;49;00m(f1, \u001b[34m3\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy_score:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mround\u001b[39;49;00m(accuracy, \u001b[34m3\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    OUTPUT_DIR = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving model....\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving model....\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    path = os.path.join(OUTPUT_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mxgb_model.pkl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msaving to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpath\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m p_file:\n",
      "        pickle.dump(final_model, p_file)\n",
      "            \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining Job is completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize customer_churn_training/model/train_without.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f1896",
   "metadata": {},
   "source": [
    "Please execute bellow code to create the pipeline. This pipeline contains **preprocessing and training** steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3603e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(\n",
    "    region,\n",
    "    subnets,\n",
    "    security_group_ids,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"CustomerChurnModelPackageGroup\",  # Choose any name\n",
    "    pipeline_name=\"customer-churn-prediction-training-pipeline\",  # You can find your pipeline name in the Studio UI (project -> Pipelines -> name)\n",
    "    base_job_prefix=\"customer-churn-prediction-training\", # Choose any name\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on CustomerChurn data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    #data versioning control using date\n",
    "    srilanka_tz = pytz.timezone('Asia/Colombo')\n",
    "    s3 = boto3.client('s3')\n",
    "    date_folder = datetime.now(srilanka_tz).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #working with input data path\n",
    "    input_data =  f\"s3://{default_bucket}/customer_churn/training_input_dataset/telco_cutomer_churn.csv\"\n",
    "    \n",
    "    #working with output data path   \n",
    "    preprocessed_output1 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output1/\"\n",
    "    preprocessed_output2 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output2/\"\n",
    "    \n",
    "    \n",
    "    sagemaker_session = get_session(region, default_bucket)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    # Parameters for pipeline execution\n",
    "    model_path = ParameterString(\n",
    "        name=\"ModelPath\",\n",
    "        default_value=f\"s3://{default_bucket}/customer_churn/training/{date_folder}/model/xgboost/\", \n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\",\n",
    "        default_value=\"PendingManualApproval\",  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    "    )\n",
    "\n",
    "    ####### --------------------- PREPROCESSING --------------------------------------------------------------------\n",
    "\n",
    "    ecr_repository = \"customer-churn-prediction-preprocessing-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    preprocessing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "        \n",
    "    script_processor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = \"ml.m5.large\",\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )\n",
    "    \n",
    "    step_preprocess = ProcessingStep(\n",
    "        name= f\"{base_job_prefix}-preprocessing\",\n",
    "        processor= script_processor, \n",
    "        code= \"customer_churn_training_preprocessing/preprocessing.py\",\n",
    "        inputs= [ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "               ],\n",
    "        outputs= [\n",
    "            ProcessingOutput(output_name=\"output1\", destination=preprocessed_output1, source=\"/opt/ml/processing/output1\"),\n",
    "            ProcessingOutput(output_name=\"output2\", destination=preprocessed_output2,  source=\"/opt/ml/processing/output2\"),       \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ###### --------------------- TRAINING --------------------------------------------------------------------\n",
    "    \n",
    "    # Training step for generating model artifacts\n",
    "    ecr_repository_est = \"customer-churn-prediction-training-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    recommender_image_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository_est + tag\n",
    "    )\n",
    "    \n",
    "    estimator = Estimator(image_uri=recommender_image_uri,\n",
    "                      role=role,\n",
    "                      sagemaker_session=sess,                                  # Technical object\n",
    "                      output_path=model_path,\n",
    "                      base_job_name=f'{base_job_prefix}-training-job',\n",
    "                      input_mode='File',                                       # Copy the dataset and then train    \n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type= \"ml.m5.4xlarge\",\n",
    "                      debugger_hook_config=False,\n",
    "                      disable_profiler = True,\n",
    "                      metric_definitions=[\n",
    "                            {'Name': 'roc_auc_score:' , 'Regex': 'roc_auc_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'Precision' , 'Regex': 'precision:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'recall_score' , 'Regex': 'recall_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'f1_score' , 'Regex': 'f1_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'accuracy_score' , 'Regex': 'accuracy_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                      ],\n",
    "                      #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Training'}],\n",
    "                      subnets = subnets.split(':'),\n",
    "                      security_group_ids = security_group_ids.split(':')\n",
    "                         )\n",
    "\n",
    "    # start training\n",
    "    step_train = TrainingStep(\n",
    "        name= f\"{base_job_prefix}-training\",\n",
    "        estimator= estimator,\n",
    "        inputs = {\n",
    "            \"input1\": TrainingInput(\n",
    "                s3_data= step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output1\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"input2\": TrainingInput(\n",
    "                s3_data= step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output2\"].S3Output.S3Uri,\n",
    "               content_type=\"text/csv\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    # ========================================= PIPELINE ORCHESTRATION ================================================\n",
    "    \n",
    "    # Pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            model_path,\n",
    "            model_approval_status\n",
    "        ],\n",
    "        steps=[\n",
    "            step_preprocess,\n",
    "            step_train,\n",
    "              ],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e437af67",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b212a8b",
   "metadata": {},
   "source": [
    "## 2.6 Define the Model Evaluation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b7d1b",
   "metadata": {},
   "source": [
    "In this step we are going to evaluate the model performace and we used accuracy, precision, f1 score and recall values to evaluate it. Later we will use these matrices for another script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import boto3\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "#import joblib\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput,TransformInput,CreateModelInput\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.conditions import (\n",
    "    ConditionGreaterThanOrEqualTo,\n",
    ")\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    TransformStep,\n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "def get_pipeline(\n",
    "    region,\n",
    "    subnets,\n",
    "    security_group_ids,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"CustomerChurnModelPackageGroup\",  # Choose any name\n",
    "    pipeline_name=\"customer-churn-prediction-training-pipeline\",  # You can find your pipeline name in the Studio UI (project -> Pipelines -> name)\n",
    "    base_job_prefix=\"customer-churn-prediction-training\", # Choose any name\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on CustomerChurn data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    #data versioning control using date\n",
    "    srilanka_tz = pytz.timezone('Asia/Colombo')\n",
    "    s3 = boto3.client('s3')\n",
    "    date_folder = datetime.now(srilanka_tz).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #working with input data path\n",
    "    input_data =  f\"s3://{default_bucket}/customer_churn/training_input_dataset/telco_cutomer_churn.csv\"\n",
    "    \n",
    "    #working with output data path   \n",
    "    preprocessed_output1 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output1/\"\n",
    "    preprocessed_output2 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output2/\"\n",
    "    preprocessed_output3 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output3/\"\n",
    "    preprocessed_output4 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output4/\"\n",
    "    \n",
    "    sagemaker_session = get_session(region, default_bucket)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    # Parameters for pipeline execution\n",
    "    model_path = ParameterString(\n",
    "        name=\"ModelPath\",\n",
    "        default_value=f\"s3://{default_bucket}/customer_churn/training/{date_folder}/model/xgboost/\", \n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\",\n",
    "        default_value=\"PendingManualApproval\",  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    "    )\n",
    "\n",
    "    ####### --------------------- PREPROCESSING --------------------------------------------------------------------\n",
    "\n",
    "    ecr_repository = \"customer-churn-prediction-preprocessing-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    preprocessing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "        \n",
    "    script_processor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = \"ml.m5.large\",\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )\n",
    "    \n",
    "    step_preprocess = ProcessingStep(\n",
    "        name= f\"{base_job_prefix}-preprocessing\",\n",
    "        processor= script_processor, \n",
    "        code= \"customer_churn_training_preprocessing/preprocessing.py\",\n",
    "        inputs= [ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "               ],\n",
    "        outputs= [\n",
    "            ProcessingOutput(output_name=\"output1\", destination=preprocessed_output1, source=\"/opt/ml/processing/output1\"),\n",
    "            ProcessingOutput(output_name=\"output2\", destination=preprocessed_output2,  source=\"/opt/ml/processing/output2\"),\n",
    "            ProcessingOutput(output_name=\"X_val\", destination=preprocessed_output3, source=\"/opt/ml/processing/X_val\"),\n",
    "            ProcessingOutput(output_name=\"y_val\", destination=preprocessed_output4, source=\"/opt/ml/processing/y_val\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ###### --------------------- TRAINING --------------------------------------------------------------------\n",
    "    \n",
    "    # Training step for generating model artifacts\n",
    "    ecr_repository_est = \"customer-churn-prediction-training-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    recommender_image_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository_est + tag\n",
    "    )\n",
    "    \n",
    "    estimator = Estimator(image_uri=recommender_image_uri,\n",
    "                      role=role,\n",
    "                      sagemaker_session=sess,                                  # Technical object\n",
    "                      output_path=model_path,\n",
    "                      base_job_name=f'{base_job_prefix}-training-job',\n",
    "                      input_mode='File',                                       # Copy the dataset and then train    \n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type= \"ml.m5.4xlarge\",\n",
    "                      debugger_hook_config=False,\n",
    "                      disable_profiler = True,\n",
    "                      metric_definitions=[\n",
    "                            {'Name': 'roc_auc_score:' , 'Regex': 'roc_auc_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'Precision' , 'Regex': 'precision:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'recall_score' , 'Regex': 'recall_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'f1_score' , 'Regex': 'f1_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'accuracy_score' , 'Regex': 'accuracy_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                      ],\n",
    "                      #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Training'}],\n",
    "                      subnets = subnets.split(':'),\n",
    "                      security_group_ids = security_group_ids.split(':')\n",
    "                         )\n",
    "\n",
    "    # start training\n",
    "    step_train = TrainingStep(\n",
    "        name= f\"{base_job_prefix}-training\",\n",
    "        estimator= estimator,\n",
    "        inputs = {\n",
    "            \"input1\": TrainingInput(\n",
    "                s3_data= step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output1\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"input2\": TrainingInput(\n",
    "                s3_data= step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output2\"].S3Output.S3Uri,\n",
    "               content_type=\"text/csv\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Processing step for evaluation\n",
    "    script_eval = ScriptProcessor(\n",
    "        image_uri=\"120582440665.dkr.ecr.ap-southeast-1.amazonaws.com/customer-churn-prediction-evaluation-image:latest\",\n",
    "        command=[\"python3\"],\n",
    "        instance_type=\"ml.m5.2xlarge\",\n",
    "        instance_count=1,\n",
    "        base_job_name=f\"{base_job_prefix}-eval\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    )\n",
    "    \n",
    "    evaluation_report = PropertyFile(\n",
    "        name=\"EvaluationReport\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\",\n",
    "    )\n",
    "    \n",
    "    step_eval = ProcessingStep(\n",
    "        name=f\"{base_job_prefix}-eval-job\",\n",
    "        processor=script_eval,\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_preprocess.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"X_val\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/X_val\",\n",
    "            ),\n",
    "             ProcessingInput(\n",
    "                source=step_preprocess.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"y_val\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/y_val\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "        ],\n",
    "        #code=os.path.join(BASE_DIR, \"evaluate.py\"),\n",
    "        code = \"customer_churn_training_evaluation/evaluation.py\",\n",
    "        property_files=[evaluation_report],\n",
    "    )\n",
    "\n",
    "    # Register model step that will be conditionally executed\n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=\"{}/evaluation.json\".format(\n",
    "                step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # ========================================= PIPELINE ORCHESTRATION ================================================\n",
    "    \n",
    "    # Pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            model_path,\n",
    "            model_approval_status\n",
    "        ],\n",
    "        steps=[\n",
    "            step_preprocess,\n",
    "            step_train,\n",
    "            step_eval,\n",
    "              ],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5a0a9",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76273417",
   "metadata": {},
   "source": [
    "## 2.7 Define the Model Register step\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ece05",
   "metadata": {},
   "source": [
    "Register your model in model registery using bellow code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3aed47",
   "metadata": {},
   "source": [
    "Please execute bellow code to create the pipeline.This pipeline contains **preprocessing,training, evaluating and model register** steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78f79e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import boto3\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "#import joblib\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput,TransformInput,CreateModelInput\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.conditions import (\n",
    "    ConditionGreaterThanOrEqualTo,\n",
    ")\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    TransformStep,\n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b09631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.functions import JsonGet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf47dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(\n",
    "    region,\n",
    "    subnets,\n",
    "    security_group_ids,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"CustomerChurnModelPackageGroup\",  # Choose any name\n",
    "    pipeline_name=\"customer-churn-prediction-training-pipeline\",  # You can find your pipeline name in the Studio UI (project -> Pipelines -> name)\n",
    "    base_job_prefix=\"customer-churn-prediction-training\", # Choose any name\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on CustomerChurn data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    #data versioning control using date\n",
    "    srilanka_tz = pytz.timezone('Asia/Colombo')\n",
    "    s3 = boto3.client('s3')\n",
    "    date_folder = datetime.now(srilanka_tz).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #working with input data path\n",
    "    input_data =  f\"s3://{default_bucket}/customer_churn/training_input_dataset/telco_cutomer_churn.csv\"\n",
    "    \n",
    "    #working with output data path   \n",
    "    preprocessed_output1 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output1/\"\n",
    "    preprocessed_output2 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output2/\"\n",
    "    preprocessed_output3 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output3/\"\n",
    "    preprocessed_output4 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output4/\"\n",
    "    \n",
    "    sagemaker_session = get_session(region, default_bucket)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    # Parameters for pipeline execution\n",
    "    model_path = ParameterString(\n",
    "        name=\"ModelPath\",\n",
    "        default_value=f\"s3://{default_bucket}/customer_churn/training/{date_folder}/model/xgboost/\", \n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\",\n",
    "        default_value=\"PendingManualApproval\",  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    "    )\n",
    "\n",
    "    ####### --------------------- PREPROCESSING --------------------------------------------------------------------\n",
    "\n",
    "    ecr_repository = \"customer-churn-prediction-preprocessing-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    preprocessing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "        \n",
    "    script_processor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = \"ml.m5.large\",\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )\n",
    "    \n",
    "    step_preprocess = ProcessingStep(\n",
    "        name= f\"{base_job_prefix}-preprocessing\",\n",
    "        processor= script_processor, \n",
    "        code= \"customer_churn_training_preprocessing/preprocessing.py\",\n",
    "        inputs= [ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "               ],\n",
    "        outputs= [\n",
    "            ProcessingOutput(output_name=\"output1\", destination=preprocessed_output1, source=\"/opt/ml/processing/output1\"),\n",
    "            ProcessingOutput(output_name=\"output2\", destination=preprocessed_output2,  source=\"/opt/ml/processing/output2\"),\n",
    "            ProcessingOutput(output_name=\"X_val\", destination=preprocessed_output3, source=\"/opt/ml/processing/X_val\"),\n",
    "            ProcessingOutput(output_name=\"y_val\", destination=preprocessed_output4, source=\"/opt/ml/processing/y_val\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ###### --------------------- TRAINING --------------------------------------------------------------------\n",
    "    \n",
    "    # Training step for generating model artifacts\n",
    "    ecr_repository_est = \"customer-churn-prediction-training-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    recommender_image_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository_est + tag\n",
    "    )\n",
    "    \n",
    "    estimator = Estimator(image_uri=recommender_image_uri,\n",
    "                      role=role,\n",
    "                      sagemaker_session=sess,                                  # Technical object\n",
    "                      output_path=model_path,\n",
    "                      base_job_name=f'{base_job_prefix}-training-job',\n",
    "                      input_mode='File',                                       # Copy the dataset and then train    \n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type= \"ml.m5.4xlarge\",\n",
    "                      debugger_hook_config=False,\n",
    "                      disable_profiler = True,\n",
    "                      metric_definitions=[\n",
    "                            {'Name': 'roc_auc_score:' , 'Regex': 'roc_auc_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'Precision' , 'Regex': 'precision:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'recall_score' , 'Regex': 'recall_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'f1_score' , 'Regex': 'f1_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'accuracy_score' , 'Regex': 'accuracy_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                      ],\n",
    "                      #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Training'}],\n",
    "                      subnets = subnets.split(':'),\n",
    "                      security_group_ids = security_group_ids.split(':')\n",
    "                         )\n",
    "\n",
    "    # start training\n",
    "    step_train = TrainingStep(\n",
    "        name= f\"{base_job_prefix}-training\",\n",
    "        estimator= estimator,\n",
    "        inputs = {\n",
    "            \"input1\": TrainingInput(\n",
    "                s3_data= step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output1\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"input2\": TrainingInput(\n",
    "                s3_data= step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output2\"].S3Output.S3Uri,\n",
    "               content_type=\"text/csv\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Processing step for evaluation\n",
    "    script_eval = ScriptProcessor(\n",
    "        image_uri=\"120582440665.dkr.ecr.ap-southeast-1.amazonaws.com/customer-churn-prediction-evaluation-image:latest\",\n",
    "        command=[\"python3\"],\n",
    "        instance_type=\"ml.m5.2xlarge\",\n",
    "        instance_count=1,\n",
    "        base_job_name=f\"{base_job_prefix}-eval\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    )\n",
    "    \n",
    "    evaluation_report = PropertyFile(\n",
    "        name=\"EvaluationReport\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\",\n",
    "    )\n",
    "    \n",
    "    step_eval = ProcessingStep(\n",
    "        name=f\"{base_job_prefix}-eval-job\",\n",
    "        processor=script_eval,\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_preprocess.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"X_val\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/X_val\",\n",
    "            ),\n",
    "             ProcessingInput(\n",
    "                source=step_preprocess.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"y_val\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/y_val\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "        ],\n",
    "        #code=os.path.join(BASE_DIR, \"evaluate.py\"),\n",
    "        code = \"customer_churn_training_evaluation/evaluation.py\",\n",
    "        property_files=[evaluation_report],\n",
    "    )\n",
    "\n",
    "    # Register model step that will be conditionally executed\n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=\"{}/evaluation.json\".format(\n",
    "                step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "    ###### --------------------- Model Registry ----------------------------------------------------------------\n",
    "    \n",
    "    #registering the model\n",
    "\n",
    "    step_register = RegisterModel(\n",
    "        name= f\"{base_job_prefix}-registermodel\",\n",
    "        estimator= estimator,\n",
    "        model_data= step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        content_types= [\"text/csv\"],\n",
    "        response_types= [\"text/csv\"],\n",
    "        inference_instances= [\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "        transform_instances= [\"ml.m5.xlarge\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        approval_status=model_approval_status,\n",
    "        model_metrics=model_metrics,\n",
    "        display_name=\"XG boost model- LR 0.05\",\n",
    "        description=\"XG boost model - only changed the learning rate to 0.05\",\n",
    "        #tags=None,\n",
    "    )\n",
    "    \n",
    "    # ========================================= PIPELINE ORCHESTRATION ================================================\n",
    "    \n",
    "    # Pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            model_path,\n",
    "            model_approval_status\n",
    "        ],\n",
    "        steps=[\n",
    "            step_preprocess,\n",
    "            step_train,\n",
    "            step_eval,\n",
    "            step_register\n",
    "              ],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e7bbe",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26f489",
   "metadata": {},
   "source": [
    "## 2.8 Define the Condition Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467253a",
   "metadata": {},
   "source": [
    "A condition step requires a list of conditions, a list of steps to run if the condition evaluates to true, and a list of steps to run if the condition evaluates to false.\n",
    "<p>Now we are going to add that step to the pipeline and in here we are going to check the model performance exceed the accuracy level 0.8 or not. If the accuracy level exceed that treshold value, it will automatically going to the defined conditions. In here it's storing the model artifact in the model registry step. The following example shows how to create a Condition step definition. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c84a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(\n",
    "    region,\n",
    "    subnets,\n",
    "    security_group_ids,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"CustomerChurnModelPackageGroup\",  # Choose any name\n",
    "    pipeline_name=\"customer-churn-prediction-training-pipeline\",  # You can find your pipeline name in the Studio UI (project -> Pipelines -> name)\n",
    "    base_job_prefix=\"customer-churn-prediction-training\", # Choose any name\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on CustomerChurn data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    #data versioning control using date\n",
    "    srilanka_tz = pytz.timezone('Asia/Colombo')\n",
    "    s3 = boto3.client('s3')\n",
    "    date_folder = datetime.now(srilanka_tz).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #working with input data path\n",
    "    input_data =  f\"s3://{default_bucket}/customer_churn/training_input_dataset/telco_cutomer_churn.csv\"\n",
    "    \n",
    "    #working with output data path   \n",
    "    preprocessed_output1 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output1/\"\n",
    "    preprocessed_output2 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output2/\"\n",
    "    preprocessed_output3 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output3/\"\n",
    "    preprocessed_output4 = f\"s3://{default_bucket}/customer_churn/training/{date_folder}/output4/\"\n",
    "    \n",
    "    sagemaker_session = get_session(region, default_bucket)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    # Parameters for pipeline execution\n",
    "    model_path = ParameterString(\n",
    "        name=\"ModelPath\",\n",
    "        default_value=f\"s3://{default_bucket}/customer_churn/training/{date_folder}/model/xgboost/\", \n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\",\n",
    "        default_value=\"PendingManualApproval\",  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    "    )\n",
    "\n",
    "    ####### --------------------- PREPROCESSING --------------------------------------------------------------------\n",
    "\n",
    "    ecr_repository = \"customer-churn-prediction-preprocessing-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    preprocessing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "        \n",
    "    script_processor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = \"ml.m5.large\",\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )\n",
    "    \n",
    "    step_preprocess = ProcessingStep(\n",
    "        name= f\"{base_job_prefix}-preprocessing\",\n",
    "        processor= script_processor, \n",
    "        code= \"customer_churn_training_preprocessing/preprocessing.py\",\n",
    "        inputs= [ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "               ],\n",
    "        outputs= [\n",
    "            ProcessingOutput(output_name=\"output1\", destination=preprocessed_output1, source=\"/opt/ml/processing/output1\"),\n",
    "            ProcessingOutput(output_name=\"output2\", destination=preprocessed_output2,  source=\"/opt/ml/processing/output2\"),\n",
    "            ProcessingOutput(output_name=\"X_val\", destination=preprocessed_output3, source=\"/opt/ml/processing/X_val\"),\n",
    "            ProcessingOutput(output_name=\"y_val\", destination=preprocessed_output4, source=\"/opt/ml/processing/y_val\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ###### --------------------- TRAINING --------------------------------------------------------------------\n",
    "    \n",
    "    # Training step for generating model artifacts\n",
    "    ecr_repository_est = \"customer-churn-prediction-training-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    recommender_image_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository_est + tag\n",
    "    )\n",
    "    \n",
    "    estimator = Estimator(image_uri=recommender_image_uri,\n",
    "                      role=role,\n",
    "                      sagemaker_session=sess,                                  # Technical object\n",
    "                      output_path=model_path,\n",
    "                      base_job_name=f'{base_job_prefix}-training-job',\n",
    "                      input_mode='File',                                       # Copy the dataset and then train    \n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type= \"ml.m5.4xlarge\",\n",
    "                      debugger_hook_config=False,\n",
    "                      disable_profiler = True,\n",
    "                      metric_definitions=[\n",
    "                            {'Name': 'roc_auc_score:' , 'Regex': 'roc_auc_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'Precision' , 'Regex': 'precision:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'recall_score' , 'Regex': 'recall_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'f1_score' , 'Regex': 'f1_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'accuracy_score' , 'Regex': 'accuracy_score:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                      ],\n",
    "                      #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Training'}],\n",
    "                      subnets = subnets.split(':'),\n",
    "                      security_group_ids = security_group_ids.split(':')\n",
    "                         )\n",
    "\n",
    "    # start training\n",
    "    step_train = TrainingStep(\n",
    "        name= f\"{base_job_prefix}-training\",\n",
    "        estimator= estimator,\n",
    "        inputs = {\n",
    "            \"input1\": TrainingInput(\n",
    "                s3_data= step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output1\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"input2\": TrainingInput(\n",
    "                s3_data= step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output2\"].S3Output.S3Uri,\n",
    "               content_type=\"text/csv\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Processing step for evaluation\n",
    "    script_eval = ScriptProcessor(\n",
    "        image_uri=\"120582440665.dkr.ecr.ap-southeast-1.amazonaws.com/customer-churn-prediction-evaluation-image:latest\",\n",
    "        command=[\"python3\"],\n",
    "        instance_type=\"ml.m5.2xlarge\",\n",
    "        instance_count=1,\n",
    "        base_job_name=f\"{base_job_prefix}-eval\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    )\n",
    "    \n",
    "    evaluation_report = PropertyFile(\n",
    "        name=\"EvaluationReport\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\",\n",
    "    )\n",
    "    \n",
    "    step_eval = ProcessingStep(\n",
    "        name=f\"{base_job_prefix}-eval-job\",\n",
    "        processor=script_eval,\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_preprocess.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"X_val\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/X_val\",\n",
    "            ),\n",
    "             ProcessingInput(\n",
    "                source=step_preprocess.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"y_val\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/y_val\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "        ],\n",
    "        #code=os.path.join(BASE_DIR, \"evaluate.py\"),\n",
    "        code = \"customer_churn_training_evaluation/evaluation.py\",\n",
    "        property_files=[evaluation_report],\n",
    "    )\n",
    "\n",
    "    # Register model step that will be conditionally executed\n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=\"{}/evaluation.json\".format(\n",
    "                step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "    ###### --------------------- Model Registry ----------------------------------------------------------------\n",
    "    \n",
    "    #registering the model\n",
    "\n",
    "    step_register = RegisterModel(\n",
    "        name= f\"{base_job_prefix}-registermodel\",\n",
    "        estimator= estimator,\n",
    "        model_data= step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        content_types= [\"text/csv\"],\n",
    "        response_types= [\"text/csv\"],\n",
    "        inference_instances= [\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "        transform_instances= [\"ml.m5.xlarge\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        approval_status=model_approval_status,\n",
    "        model_metrics=model_metrics,\n",
    "        display_name=\"XG boost model- LR 0.05\",\n",
    "        description=\"XG boost model - only changed the learning rate to 0.05\",\n",
    "        #tags=None,\n",
    "    )\n",
    "   \n",
    "    # Condition step for evaluating model quality and branching execution\n",
    "    cond_lte = ConditionGreaterThanOrEqualTo(  # You can change the condition here\n",
    "        left=JsonGet(\n",
    "            step_name=step_eval.name,\n",
    "            property_file=evaluation_report,\n",
    "            json_path=\"binary_classification_metrics.accuracy.value\",  # This should follow the structure of your report_dict defined in the evaluate.py file.\n",
    "        ),\n",
    "        right=0.8,  # You can change the threshold here\n",
    "    )\n",
    "    step_cond = ConditionStep(\n",
    "        name=f\"{base_job_prefix}-accuracycond\",\n",
    "        conditions=[cond_lte],\n",
    "        if_steps=[step_register],\n",
    "        else_steps=[],\n",
    "    )\n",
    "    \n",
    "    # ========================================= PIPELINE ORCHESTRATION ================================================\n",
    "    \n",
    "    # Pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            model_path,\n",
    "            model_approval_status\n",
    "        ],\n",
    "        steps=[\n",
    "            step_preprocess,\n",
    "            step_train,\n",
    "            step_eval,\n",
    "            #step_register\n",
    "            step_cond\n",
    "              ],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b659175",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval9.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c784e",
   "metadata": {},
   "source": [
    "Addtional if we need we can use stop/pipeline failed condition by adding below command to the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.parameters import ParameterInteger\n",
    "\n",
    "mse_threshold_param = ParameterInteger(name=\"accuracyThreshold\", default_value=0.8)\n",
    "step_fail = FailStep(\n",
    "    name=\"AbaloneMSEFail\",\n",
    "    error_message=Join(\n",
    "        on=\" \", values=[\"Execution failed due to Accuracy >\", mse_threshold_param]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e049f3",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1168d40",
   "metadata": {},
   "source": [
    "## 2.9 Define required parameters for get pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cfca14",
   "metadata": {},
   "source": [
    "Define subnets and parameters for get_pipeline function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnets = 'your_subnet'\n",
    "sg = 'your security group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7009cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "role='your IAM role'\n",
    "#role=None\n",
    "default_bucket=default_bucket\n",
    "pipeline_def = get_pipeline(region, \n",
    "                            subnets, \n",
    "                            sg, \n",
    "                            role,\n",
    "                            default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2695e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_def.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f235224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline_def.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731da786",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f56d3d",
   "metadata": {},
   "source": [
    "![workflowimage](images/eval11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec4adb4",
   "metadata": {},
   "source": [
    "#### To see the execution ID\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614b6cb",
   "metadata": {},
   "source": [
    "## Lineage\n",
    "Review the lineage of the artifacts generated by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
